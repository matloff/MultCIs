

---
title: "Forming Multiple Confidence Intervals for Model Selection"
author: Norm Matloff
toc: true
---

[Author bio](https://heather.cs.ucdavis.edu/matloff.html)

[]{.column-margin} In developing/choosing a model, we may perform
simulation or cross-validation, with many replicates, in order to
compare multiple algorithms, multiple sets of hyperparameters and so on.
To make our analysis statistically valid, we can form confidence
intervals (CIs).  

However, if we form many CIs, say at 95% level each, their overall
coverage probability will be much lower than 95%.  This is the *multiple
inference* (MI) or *multiple comparisons* problem, sometimes also called
*simultaneous inference*. In this document, we discuss remedies in the
model-selection context. 

[For a good general presentation of MI methods, see Jason Hsu, *Multiple
Comparisons: Theory and methods*.]{.column-margin} The MI problem has
been extremely well studied, resulting in myriad methods.  Here we
employ two of the most well-known methods, the Bonferroni Inequality and
Scheffe's Method.

Note that our focus is on CIs, not hypothesis tests. We strongly
[recommend against](https://matloff.github.io/No-P-Values/NPV.htm)
the latter approach.

# Motivating Example

``` r
library(qeML)
data(svcensus)
head(svcensus)
```

This is US census data. Let's predict gender.

``` r
logitAcc <- qeLogit(svcensus,'gender')$testAcc
rfAcc <- qeRFranger(svcensus,'gender')$testAcc
xgbAcc <- qeXGBoost(svcensus,'gender')$testAcc
knn25Acc <- qeKNN(svcensus,'gender',k=25)$testAcc
knn200Acc <- qeKNN(svcensus,'gender',k=200)$testAcc
c(logitAcc,rfAcc,xgbAcc,knn25Acc,knn200Acc)
# 0.263 0.254 0.263 0.256 0.236
```

Several points to note:

* The **qeML** functions automatically do cross-validation, via
  an argument **holdout**, indicating our desired size of test set.
  Here we take the default value. 

* The functions return S3 objects, one of whose components is prediction
  accuracy on the test data, **testAcc**, in this case the probability
  of misclassification..

* Since the random number seed is not reset, each of the calls
  is using different training sets and different test sets
  from each other. This makes them statistically independent. The
  alternative (not necessarily better or worse) would be to insert, say

  ``` r
  set.seed(9999)
  ```

  before each of the calls.

* Since the holdout set is random, we should be performing each of the
  calls many times, and compute averages, say

  ``` r
  logitAccs <- 
     replicate(50,qeLogit(svcensus,'gender')$testAcc)
  rfAccs <- 
     replicate(50,qeRFranger(svcensus,'gender')$testAcc)
  xgbAccs <- 
     replicate(50,qeXGBoost(svcensus,'gender')$testAcc)
  knn25Accs <- replicate(50,qeKNN(svcensus,'gender',k=25)$testAcc)
  knn200Accs <- replicate(50,qeKNN(svcensus,'gender',k=200)$testAcc)
  accs <- cbind(logitAccs,rfAccs,xgbAccs,knn25Accs,knn200Accs)
  colMeans(accs)
  # 0.24476    0.26124    0.25828    0.25194    0.24844
  ```


We might wish to find a CI for each of the 5 quantities, or for each
difference, i.e. c(5,2) = 10 CIs. With more algorithms, and especially
with more hyperparameter combinations, our CI count could easily be
several dozen or more, raising MI concerns.

# Review: Confidence Intervals, Standard Errors

[We follow the standard model in statistics in which one's data are
considered from some population, actual or conceptual. In the ML
community, the term *data-generating process* is
analogous.]{.column-margin} To set the stage, let's review the
statistical concepts of *confidence interval* and *standard error*. Say
we have an estimator $\widehat{\theta}$ of some population parameter
$\theta$, e.g.\ $\bar{X}$ for a population mean $\mu$.

Loosely speaking, the term *standard error* of is our estimate of
$\sqrt{\hat{Var}(\widehat{\theta})}$.  More precisely, suppose that
$\widehat{\theta}$ is asymptotically normal/Gaussian.  The standard
error is an estimate of the standard deviation of that normal
distribution. For this reason, one sometimes writes
$AVar(\widehat{\theta})$ rather than $Var(\widehat{\theta})$. 

[Standard statistics courses begin with "exact" CIs, using the
t-distribution, which assumes $X$ has a normal dstr. But nothing in
practice is normally distributed; no one is 80 feet tall, for
instance]{.column-margin} In the familiar case in which $\theta$ is a
population mean and $\widehat{\theta}$ is the sample mean, 

$$
AVar(\widehat{\theta}) = \frac{s}{\sqrt{n}}
$$

where $s$ is the sample standard deviation.

A, say 95%, confidence interval (CI) for $\theta$ is then
$$
\widehat{\theta} \pm 1.96 ~ \textrm{SE}(\widehat{\theta})
$$

where we denote the standard error of $\widehat{\theta}$ by
$\textrm{SE}(\widehat{\theta})$.

The 95% figure means that of all possible samples of the given size from
the population, 95% of the resulting confidence intervals will contain
$\theta$. In many cases, the 95% figure is only approximate, stemming
from a derivation that uses the Central Limit Theorem.

In general, for confidence level $1-\alpha$, replace 1.96 by
$z_{\alpha}$, the $1-\alpha/2$ quantile of the N(0,1) distribution, Then
our CI is

$$
\widehat{\theta} \pm z_{\alpha} \textrm{SE}(\widehat{\theta})
$${#eq-pmse}

Examples of finding $z_{\alpha}$:

``` r
> qnorm(0.975)
[1] 1.959964  # for 95% CI
> qnorm(0.025)  # N(0,1) is symmetric around 0
[1] -1.959964
> qnorm(0.995)  # for 99% CI
[1] 2.575829
```



### Example: Logistic Regression Coefficients

```{r}
suppressPackageStartupMessages(library(qeML))
data(svcensus)
logitOut <- qeLogit(svcensus,'gender',yesYVal='female') 
summary(logitOut$glmOuts[[1]]) 
```

So a 95% CI for the coefficient for occupation 141 is

$$
-1.44 \pm 1.96 \times 0.07
$${#eq-196}

# The Bonferroni Inequality

This one is the simplest and most convenient MI method. The derivation
is instructive.

Suppose $A$ and $B$ are events defined on some probability space. Then

$$
P(A \textrm{ or } B) \leq P(A) + P(B)
$${#eq-probAorB}

Say we form 95% CIs for two different quantities, so that each has a
probability 5% of being "wrong," i.e.  of failing to contain its
corresponding population parameter. Set $A$ to be the event that the
first CI fails in that regard, and define $B$ similarly for the
second CI. Then @eq-probAorB tells us that the probability of at least
one of the CIs being wrong is at most 10%. In other words, *our overall
confidence level is at least 90%.*

Moreover: Presumably the reason we set the original CI levels to 95% was
that we are comfortable with an error rate of 5%. Accordingly, we may
wish to have an *overall* rate of 5% -- again, meaning that we are 95%
confident that *both* CIs are correct -- rather than the 10% shown
above. The same reasoning as above shows that we can achieve overall 95%
confidence by making each of the two individual CIs at the 97.5% level.

So we could form intervals for say both **occ140** and **occ141** above.

Of course, this MI benefit comes at a price:

``` r
qnorm((1-0.975)/2)
# -2.241403
```

So, the 1.96 in @eq-pmse now becomes 2.24, forcing us to form wider
intervals.

If in @eq-probAorB one replaces $B$ by $B_1 or B_2$, that extends the
relation from two events to three, and so on.

# Scheffe's Method

The Bonferroni Method is fine for forming a few CIs, but is impractical
if one needs many. The coefficient for forming a CI -- 1.96 and 2.24
above -- becomes too large. By contrast, the Scheffe' Method actually
gives us the freedom to form infinitely many CIs.

## The Chi-Square Family of Probability Distributions

Some readers may have seen this distribution family in the context of
goodness-of-fit tests. Here is the technical definition:

The random variable $W$ is said to have a *chi-square distribution* with
$k$ *degrees of fredom* if

$$
W = Z_{1}^2 + ... + Z_{k}^2
$$

for independent $Z_i$ having N(0,1) distributions.

So, unlike the 2-parameter family of normal distributions, chi-square
has just 1 parameter, the degrees of freedom. Note too that it is not
symmetric.

## Covariance Matrices

### Definitions

The term *covariance* is overloaded, with both scalar and matrix
versions.

* The covariance between random variables $U$ and $V$ is

  $$
  Cov(U,V) = E[(U - EU)(V - EV)]
  $$

* The *covariance matrix* of a random vector $X$ has as its row $i$,
  column $j$ element the scalar covariance between elements $i$ and $j$
  (denoted here by superscipts):

  $$
  Cov(X)_{ij} = Cov(X^{(i)},X^{(j)})
  $$

  One can show that

  $$
  Cov(X) = E[(X-EX) (X-EX)']
  $${#eq-covdef}

  where ' denotes matrix transpose and vectors column vectors by
  default.  Here $EX$ is a vector whose element $i$ is $EX^{(i)}$.

### Estimation via sample analog ("plug-in" estimates)

Denote our data by $X_1,...,X_n$. @eq-covdef is the average value of
$(X-EX) (X-EX)$ in the population, and its sample analog is the average
value in the sample, i.e.

$$
\frac{1}{n}
\sum_{i=1}^n
(X_i - \bar{X})
(X_i - \bar{X})'
$$

where

$$
\bar{X} = 
\frac{1}{n}
\sum_{i=1}^n
X_i
$$

### Estimation via parametric model

## Scheffe' CIs

# Notes

* Some analysts in some applications may not consider MI to be a
  "problem." This is a philosophical issue, not pursued here, but we
  note that the choice may depend on the application. A medical research
  journal may require MI, say, whereas an amateur stock market investor
  may not feel it's necessary for that type of data analysis.

* There is a bit of drama in this word *contain* in the phrase "will
  contain $\theta$." Instead of saying the intervals *contain* $\theta$,
  why not simply say $\theta$ is *in* the intervals? Aren't these two
  descriptions equivalent in terms of English?  Of course they are.

  But many instructors of statistics classes worry that students will
  take the description based on "in" to mean that $\theta$ is the random
  quantity, when in fact the CI is random (random center, random radius)
  and $\theta$ is fixed (though unknown). The instructors thus insist on
  the more awkward phrasing "contain," so as to avoid students
  misunderstanding. Indeed some instructors would contend that use of
  the word *in* is itself just plain incorrect.

  My own view is that in some cases the word *in* is clearer (and
  certainly correct in any case), and that it is better to add a warning
  about what is random/nonrandom than engage in awkward phrasing.

